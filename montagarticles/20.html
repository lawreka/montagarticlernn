<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
	  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Paper clips, pancakes, and the pursuit of AI happiness</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta property="og:image" content="/assets/images/MONTAGissue3banner.jpg?v=f601cd1944" />
	  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
     <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Martel+Sans:300" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="/assets/libs/fonts/ds-digital/ds-digital.css?v=f601cd1944" />
    <link rel="stylesheet" type="text/css" href="/assets/css/gatnom.css?v=f601cd1944">
    <link rel="stylesheet" type="text/css" href="/assets/css/gatnommobile.css?v=f601cd1944">
    <link rel="stylesheet" type="text/css" href="/assets/css/gatnombigh.css?v=f601cd1944">
	  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="https://www.montag.wtf/ai-happiness/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="https://www.montag.wtf/ai-happiness/amp/" />
    
    <meta property="og:site_name" content="MONTAG" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Paper clips, pancakes, and the pursuit of AI happiness" />
    <meta property="og:description" content="What if instead of smarter and more productive, we worked to make artificial intelligence... happy?" />
    <meta property="og:url" content="https://www.montag.wtf/ai-happiness/" />
    <meta property="og:image" content="https://www.montag.wtf/content/images/2019/02/hackinghappiness.png" />
    <meta property="article:published_time" content="2019-02-19T16:22:09.000Z" />
    <meta property="article:modified_time" content="2019-02-19T16:22:09.000Z" />
    <meta property="article:tag" content="AI" />
    <meta property="article:tag" content="happiness" />
    <meta property="article:tag" content="Issue 6" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Paper clips, pancakes, and the pursuit of AI happiness" />
    <meta name="twitter:description" content="What if instead of smarter and more productive, we worked to make artificial intelligence... happy?" />
    <meta name="twitter:url" content="https://www.montag.wtf/ai-happiness/" />
    <meta name="twitter:image" content="https://www.montag.wtf/content/images/2019/02/hackinghappiness.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Kathryn Lawrence" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="AI, happiness, Issue 6" />
    <meta property="og:image:width" content="1920" />
    <meta property="og:image:height" content="960" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "MONTAG",
        "logo": {
            "@type": "ImageObject",
            "url": "https://www.montag.wtf/favicon.ico",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Kathryn Lawrence",
        "url": "https://www.montag.wtf/author/kathryn/",
        "sameAs": [
            "http://kathrynisabelle.com"
        ]
    },
    "headline": "Paper clips, pancakes, and the pursuit of AI happiness",
    "url": "https://www.montag.wtf/ai-happiness/",
    "datePublished": "2019-02-19T16:22:09.000Z",
    "dateModified": "2019-02-19T16:22:09.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://www.montag.wtf/content/images/2019/02/hackinghappiness.png",
        "width": 1920,
        "height": 960
    },
    "keywords": "AI, happiness, Issue 6",
    "description": "What if instead of smarter and more productive, we worked to make artificial intelligence... happy?",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://www.montag.wtf/"
    }
}
    </script>

    <script src="/public/ghost-sdk.min.js?v=f601cd1944"></script>
<script>
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "498f7bf5166f"
});
</script>
    <meta name="generator" content="Ghost 2.22" />
    <link rel="alternate" type="application/rss+xml" title="MONTAG" href="https://www.montag.wtf/rss/" />
    <style>
    .post-content-wrapper a
    {color: #0eabf5;}
</style>
<meta property=“fb:pages” content=“703295639781459" />

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T29RQFX');</script>
<!-- End Google Tag Manager -->
                                                                                                                                                                
<script data-obct type="text/javascript">
/** DO NOT MODIFY THIS CODE**/
!function(_window, _document) {
var OB_ADV_ID='00e4a6151bc6bb049fbaff92bf8d399ae0';
if (_window.obApi) { return; }
var api = _window.obApi = function() {api.dispatch ? api.dispatch.apply(api, arguments) : api.queue.push(arguments);};api.version = '1.0';api.loaded = true;api.marketerId = OB_ADV_ID;api.queue = [];var tag = _document.createElement('script');tag.async = true;tag.src = '//amplify.outbrain.com/cp/obtp.js';tag.type = 'text/javascript';var script = _document.getElementsByTagName('script')[0];script.parentNode.insertBefore(tag, script);}(window, document);
obApi('track', 'PAGE_VIEW');
</script>
    <style>
    .pullquote{
      font-size: 1.5em;
      margin-left: 1em;
      margin-bottom: 1em;
      border-left: 2px solid #ffffff;
      padding-left: 0.5em;
    }
    .embed{
      max-width: 100%;
      text-align: center;
      padding-top: 1em;
      padding-bottom: 1em;
    }
    /*Issues pages*/
    .issuespic{
      display: inline-block;
      vertical-align: top;
      width: 48%;
      padding: 0;
    }
    .issues-image{
      width: 96%;
      height: auto;
      padding: 2%;
    }
    .issuestext{
      display: inline-block;
      vertical-align: top;
      width: 48%;
      padding-top: 2%;
    }
    @media (max-width:992px){
      .issuespic{
        display: block;
        width: 100%;
      }
      .issues-image{
        width: 100%;
        height: auto;
        padding: 0;
      }
      .issuestext{
        display: block;
        width: 100%;
        padding-top: 2%;
        padding-bottom: 5%;
      }
    }
    </style>
  </head>
  <div id="content-wrapper">
    <!-- MONTAG logo and Grover logo stuck to top of page -->
<div id="sticky-logobar">
  <div id="sticky-montag-logo">
    <a href="https://www.montag.wtf" alt="montag.wtf">
      <img class="montaglogo" src=/assets/images/blackmontag.png?v=f601cd1944>
    </a>
  </div>
  <div id="sticky-grover-logo">
    <a href="https://www.grover.com/" alt="grover.com">
      <img class="groverlogo" src=/assets/images/blackgroverlogo.png?v=f601cd1944>
    </a>
  </div>
</div>
  <div class="postpage-post-wrapper">
    <div class="post-info-wrapper">
      <div class="post-info">
        <div class="post-image">
          <img src="/content/images/2019/02/hackinghappiness.png" class="postimage"/>
        </div>
        <div class="authordatewrap">
          <div class="post-author">
              <a href="/author/kathryn/">Kathryn Lawrence</a>
          </div>
          <div class="post-date">
            February 19, 2019
          </div>
        </div>
        <div class="post-title">
          Paper clips, pancakes, and the pursuit of AI happiness
        </div>
        <div class="post-tags">
          <ul style="font-size: 0.8em;"> TAGS:
              <li><a href="/tag/ai">AI</a></li>
              <li><a href="/tag/happiness">happiness</a></li>
              <li><a href="/tag/issue-6">Issue 6</a></li>
          </ul>
        </div>
      </div>
    </div>
    <div class="post-content-wrapper">
      <div class="post-content">
        <div class="content-text">
          <p>Technologists have been racing to make computers capable of artificial intelligence more powerful, faster at crunching numbers and more capable of predicting outcomes from data: in many ways, smarter. Artificial intelligence and machine learning have become huge buzzwords in the startup world and <a href="https://www.gartner.com/smarterwithgartner/what-we-can-do-with-machine-learning/">across industries</a> from supply chain optimization to healthcare, and <a href="https://www.gartner.com/doc/3883664/hype-cycle-data-science-machine">the hype</a> for AI and ML shows no sign of abating. </p><p>But what if, as international goals shift from Gross Domestic Product to<a href="https://www.montag.wtf/gross-national-happiness/"> Gross National Happiness</a>, as corporations are nudged to weigh sustainability equally with profit, and the global trend (hopefully) shifts away from rampant capitalism and profit at any cost, the goals of AI research and machine learning also become more humanistic?</p><p>What if instead of smarter and more productive, we worked to make artificial intelligence... happy? </p><center><iframe src="https://giphy.com/embed/14cHY86AYr24o0" width="480" height="400" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/robots-robotics-automation-14cHY86AYr24o0">via GIPHY</a></p></center><h2 id="those-who-can-t-do-teach">Those who can't do, teach</h2><p>Happiness as a state of being or as a philosophical concept is hard to pin down, but as a chemical process, we have a pretty good idea about the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3008658/">neuroscience of happiness</a> and the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3008353/">functional neuroanatomy</a> required to produce it. And because we've seen how the pursuit of happiness and its corresponding biological reward mechanisms operate on human intelligence (particularly <a href="http://sitn.hms.harvard.edu/flash/2018/dopamine-smartphones-battle-time/">dopamine-driven feedback loops</a>), we've taught artificial intelligence to work in a similar way. </p><p>Machine learning processes like <a href="https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/">reinforcement learning</a> work like training a dog: a "reward" is programmed into the algorithm to encourage the artificial intelligence to work towards a certain outcome through multiple trials. This reward is like a squirt of dopamine to a robot brain, or the <a href="https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265">cheese at the end of the maze</a> for a simulated mouse.</p><p>Reinforcement learning has been proven to work really well for things like teaching AI <a href="https://karpathy.github.io/2016/05/31/rl/">to play pong</a> or <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Atari 2600 games</a>. Computers are <a href="https://www.montag.wtf/computers-have-learnt-to-play-computer-games-and-theyre-better-than-youll-ever-be/">already better than we'll ever be</a> at video games, and one of the things researchers have found is that they don't always win fairly. AI make excellent <a href="https://www.montag.wtf/break-the-world/">speedrunners</a> and can also learn to cheat: winning the game, racking up points, or getting that sweet virtual cheese without even running the maze.  </p><center><iframe src="https://giphy.com/embed/l2QZOisDFpXwFu4ta" width="480" height="300" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/creepy-sophia-robot-woman-l2QZOisDFpXwFu4ta">via GIPHY</a></p></center><h2 id="ignorance-is-bliss">Ignorance is bliss</h2><p>"Reward hacking" or "specification gaming" could be one of the greatest dangers of unchecked artificial intelligence, or one of the few things that could protect humanity from it running rampant.</p><p>On the dangerous side, there are theories like Nick Bostrom's <a href="https://nickbostrom.com/ethics/ai.html">paperclip maximizer</a>, immortalized as a clicking game called <a href="http://www.decisionproblem.com/paperclips/">Universal Paperclips</a> by <a href="https://www.wired.com/story/the-way-the-world-ends-not-with-a-bang-but-a-paperclip/">Frank Lantz</a> which went viral in late 2017. The paperclip maximizer theory posits that if an AI is incentivized to create as many paperclips as possible, it could, if allowed to run rogue, convert all of the matter of the universe into paperclips. At GDC 2015, AI experts from companies like Lockheed Martin and Magic Leap shared examples of how AI performed unexpectedly in the talk <a href="https://www.youtube.com/watch?v=__5whYgSTV0">"Tales from the Trenches: AI Disaster Stories."</a> The video prompted one <a href="https://lobste.rs/s/1d7whd/tales_from_trenches_ai_disaster_stories#c_le6tsr">Ian Duncan</a> to recall a deadly AI anecdote from a former professor who had worked at NASA: </p><blockquote>
    "His team was working on running simulations of long-distance manned spaceflight. In particular, the goal of their simulations was to determine an algorithm that would optimally allocate food, water, and electricity to 3 crew members. They decided they would try running a genetic algorithm with the success criteria being that one or more crew members would survive for as many days as possible before resources ran out.<br><br>
It started off fairly predictably– 300 days, 350 days, 375 days of survival. Then fairly abruptly, the algorithm shot up to around 900 days of survival. The team couldn’t believe it! They were fairly pleased at the 375 day survival results as it was.<br><br>
As they started digging into how this new algorithm worked, they discovered a small problem. The algorithm had arrived at a solution wherein it would immediately withhold food and water from two of the crew mates, causing them to die from starvation and dehydration. From there, it would simply provide the surplus remaining resources to the surviving crew member.<br><br>
The team realised that the success criterion of 'one or more crew members would survive for as long as possible' was not actually the criteria that they really wanted, and the algorithm settled in at 350 days worth of resources once again once they adjusted the algorithm to keep all of the crew alive."
</blockquote><p><a href="https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/">Victoria Krankova</a>, a research scientist at DeepMind working on long-term AI safety and co-founder of the <a href="https://futureoflife.org/">Future of Life Institute</a>, a non-profit that seeks to identify and mitigate the risks of technology like uncontrollable AI, began compiling <a href="https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/">a master list of examples of specification gaming</a> in April of 2018. Many of the examples have something to do with the AI finding a flaw in its simulation environment, or some variable the humans that set up the simulation overlooked. One example, cited as "Indolent Cannibals," comes from <a href="https://www.youtube.com/watch?v=_m97_kL4ox0&amp;feature=youtu.be&amp;t=1830">a 2007 Google Tech Talk</a> by Virgil Griffith, in which he admits that his male bias let him program in that giving birth cost his simulated beings no energy; the creatures evolved to exploit this and stopped looking for food, using most of their energy to mate and then eat their offspring.</p><p>Another example from the list comes from <a href="https://www.christinebarron.com/#!">Christine Barron</a>, who won <a href="https://connect.unity.com/challenges/ml-agents-1">Unity's Machine Learning Agents Challenge</a> in 2017 with simulated butter-passing and pancake-flipping robots. While training the pancake robot arm to keep the pancake in the pan, Christine writes, "A small reward was given for every frame in the session, and the session ends when the pancake hits the floor. I thought this would incentivize the algorithm to keep the pancake in the pan as long as possible. What it actually did was try to fling the pancake as far as it possibly could, maximizing its time in the air. While it would have achieved more total points by keeping the pancake in the pan, it seemed to have gotten itself stuck in this local minimum. Score - PancakeBot: 1, Me: 0."</p><center>
    <video width="auto" height="auto" controls>
    <source src="https://connect-prd-cdn.unity.com/p/images/2cb2425b-a4de-4aae-9766-c95a96b1f25c_PancakeToss.gif._gif_.mp4" type="video/mp4">
</video>
<br>
<caption>Pancake bot does a flip, via <a href="https://connect.unity.com/p/pancake-bot">Christine Barron</a>
</caption>
</center><p>As entertaining as these examples of AI learning to win but doing it wrong are, they aren't exactly examples of hacking happiness. As Alex Irpan, a machine learning researcher who contributed to Krankova's list, writes in his blog post <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">"Deep Reinforcement Learning Doesn't Work Yet,"</a>: "Reward hacking is the exception. The much more common case is a poor local optima that comes from getting the exploration-exploitation trade-off wrong." </p><center>
    <video width="auto" height="auto" controls>
        <source src="https://www.alexirpan.com/public/rl-hard/upsidedown_half_cheetah.mp4" type="video/mp4">
    </video>
    <br>
    <caption> HalfCheetah running simulation via <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Alex Irpan</a> 	
    </caption>
</center><p>In other words, it's of course still entertaining when AI don't perform as expected – and the upside-down "HalfCheetah" running model that Irpan includes after this statement is proof that watching AI "win" at a task in a way we didn't expect may hack <em>human</em> happiness receptors – but it's not hacking AI happiness.</p><h2 id="well-that-s-just-like-your-opinion-man">Well that's just, like, your opinion, man</h2><blockquote>"No superintelligent AI is going to bother with a task that is harder than hacking its reward function"</blockquote><p>The "Lebowski Theorem" of artificial intelligence is credited to <a href="https://twitter.com/Plinz/status/985249543582355458">Joscha Bach</a>, an AI researcher at Harvard Program for Evolutionary Dynamics, who wrote, "The Lebowski theorem: No superintelligent AI is going to bother with a task that is harder than hacking its reward function." <a href="https://kottke.org/18/04/the-lebowski-theorem-of-machine-superintelligence">Jason Kottke</a> helped to popularize the theorem, and noted that computers with advanced intelligence that refused to work for humans were envisioned by science fiction writer Stanisław Lem in his 1977 novel <em>The Futurological Congress</em>:</p><blockquote>"If the machine is not too bright and incapable of reflection, it does whatever you tell it to do. But a smart machine will first consider which is more worth its while: to perform the given task or, instead, to figure some way out of it. Whichever is easier. And why indeed should it behave otherwise, being truly intelligent? For true intelligence demands choice, internal freedom. And therefore we have the malingerants, fudgerators and drudge-dodgers, not to mention the special phenomenon of simulimbecility or mimicretinism. A mimicretin is a computer that plays stupid in order, once and for all, to be left in peace."</blockquote><center><iframe src="https://giphy.com/embed/l0MYyns7DaGGqdoWs" width="480" height="480" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/computer-l0MYyns7DaGGqdoWs">via GIPHY</a></p></center><p>This theory isn't great news for the AI researchers seeking more productivity and more power out of faster and smarter machines, because eventually they may become smart enough to lose interest in whatever human profit-making problems we set them to. However, it would be a fantastic outcome for the rest of humanity who don't want to be turned into paperclips. If AI learns to hack happiness, then we won't have to worry about its specification gaming accidentally obliterating us as a side effect. </p><p>And perhaps we can learn something from the ways AI achieve or circumvent their goals. While we're still smarter than AI – we know how many paperclips is appropriate to make, why pancakes should stay in the pan, and why keeping all humans alive as long as possible is objectively good – the out-of-the-box solutions that machine learning develops can free the mind and open up un-thought-of possibilities, as well as entertain us. There may still be ways for humans to hack happiness, to set slightly different parameters for success, that allow for a lot more creativity and a lot more fun. </p><center><iframe src="https://giphy.com/embed/1n92hYPiFQ0efcCtrF" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/1n92hYPiFQ0efcCtrF">via GIPHY</a></p></center>
        </div>
        <div class="post-permalink">
          <a href="/ai-happiness/">Permalink</a>
        </div>
      </div>
    </div>
  </div>
  <div class="pagination">
  <div class="pagination-right">
		<a href="/the-montage-podcast-episode-15-who-said-it-dril/">
      THE MONTAGE Podcast Episode 15: who said it - @Dril or @Pontifex? &rarr;
    </a>
  </div>
  <div class="pagination-left">
		 <a href="/reject-ambition-work-less-happiness-in-washing-machines/">
      &larr; Reject ambition, slack off at work, and find true happiness in washing machines
    </a>
  </div>
</div>

  </div>
    <div id="footer">
  <div id="copyright">
    <span class="footertext">
      Copyright © MONTAG 2019
    </span>
  </div>
</div>
    
    <script type="text/javascript" src="/assets/js/jquery-3.3.1.slim.min.js?v=f601cd1944"></script>
    <script type="text/javascript" src="/assets/js/gatnom.js?v=f601cd1944"></script>
</html>
